{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eaaa2e0-a0da-4abf-87ac-f52da24fe778",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a77eb33-e2a0-4e0d-a3be-4ba1fa0e1907",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import librosa as lb\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import (\n",
    "    WhisperProcessor,\n",
    "    WhisperFeatureExtractor,\n",
    "    WhisperTokenizerFast,\n",
    "    WhisperForConditionalGeneration,\n",
    ")\n",
    "import evaluate\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.cuda import empty_cache\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f206394e-4b7d-4ebb-a73a-64a886cdb692",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_id = 'openai/whisper-large-v2'\n",
    "processor = WhisperProcessor.from_pretrained(model_id)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_id)\n",
    "\n",
    "tokenizer = WhisperTokenizerFast.from_pretrained(model_id)\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00de64e8-478d-40a9-b7c3-fa61c2f1fb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "forced_decoder_ids = processor.get_decoder_prompt_ids(task=\"transcribe\", language='Hindi')\n",
    "prompt_ids = processor.get_prompt_ids('Glossary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1aca3b-a06b-47fa-9795-8b95af268c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b3b360-8864-4f73-a085-2e9be6e498f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"forced_decoder_ids\": forced_decoder_ids, # What decoder IDs to use\n",
    "    \"prompt_ids\": None, # What prompt IDs to use\n",
    "    \"num_beams\": 1, # Number of beams to use for beam search\n",
    "    \"return_timestamps\": True,\n",
    "    \"chunk_length_s\": 30\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb0e528-1dbb-4f48-a66e-4dfb9588f8a6",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "Load a toy dataset to evalute the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fbe4c2-08cd-4758-afb8-04b9c2a509d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "common_voice = DatasetDict()\n",
    "\n",
    "common_voice[\"train\"] = load_dataset(\n",
    "    \"mozilla-foundation/common_voice_11_0\", \"hi\", split=\"train+validation\"\n",
    ")\n",
    "common_voice[\"test\"] = load_dataset(\n",
    "    \"mozilla-foundation/common_voice_11_0\", \"hi\", split=\"test\"\n",
    ")\n",
    "\n",
    "print(common_voice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3770ad97-861b-4608-a0ed-693ff29e3802",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "duration = []\n",
    "for e in common_voice[\"train\"]:\n",
    "    duration.append(e[\"audio\"][\"array\"].shape[0]/e[\"audio\"][\"sampling_rate\"])\n",
    "\n",
    "duration = np.array(duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd1a3b7-c51c-43e1-abf7-d65c80409cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f'Median length of the audio: {np.percentile(duration, 0.5)}\\n95th percentile lenght of audio: {np.percentile(duration, 0.95)}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e5b9f4-aa36-4e48-9282-17d0f9eb4699",
   "metadata": {},
   "source": [
    "Since single audio clips are quite small, we combine them to form longer audio files. We will concatenate 300 audio files one after the other. Each audio files will be also be transformed by removing the silences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b56526-ff58-4770-a4dd-525fedd5f68b",
   "metadata": {},
   "source": [
    "### Transform dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae4ad82-2e97-40aa-8928-783895309d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_silences(audio: np.ndarray, silence_model, silence_threshold:float = 0.5, **kwargs):\n",
    "    \"\"\"\n",
    "    Removes silences from the audio file\n",
    "    \"\"\"\n",
    "    get_speech_timestamps = kwargs.get('get_speech_timestamps')\n",
    "    collect_chunks = kwargs.get('collect_chunks')\n",
    "\n",
    "    speech_audio = get_speech_timestamps(\n",
    "        audio, silence_model, threshold=silence_threshold, sampling_rate=16000\n",
    "    )\n",
    "\n",
    "    speech_audio = collect_chunks(speech_audio, torch.tensor(audio))\n",
    "\n",
    "    return speech_audio.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c05541-e36a-44a5-b2cc-971c4d51638e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vad_model, vad_utils = torch.hub.load(\n",
    "    repo_or_dir='snakers4/silero-vad',\n",
    "    model='silero_vad',\n",
    "    force_reload=True,\n",
    ")\n",
    "\n",
    "(get_speech_timestamps, _, _, _, collect_chunks) = vad_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0683a9-940a-4a32-8e50-044d5ef2b96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_cut_silences = partial(\n",
    "    cut_silences, silence_model=vad_model, get_speech_timestamps=get_speech_timestamps, collect_chunks=collect_chunks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf94ee14-e078-4569-b32f-2eb9f9827728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print one example of the dataset\n",
    "common_voice[\"train\"][0].keys()\n",
    "common_voice[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216fe8c2-96d1-46d8-99d1-fbaad07da42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAudioDataset(Dataset):\n",
    "    def __init__(self, raw_ds: Dataset, concat_n:int = 10, transform: callable = None):\n",
    "        self.raw_ds = raw_ds\n",
    "        self.transform = transform\n",
    "        self.concat_n = concat_n\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_ds)//self.concat_n\n",
    "\n",
    "    def _resample_audio(self, x: np.ndarray, sr: int):\n",
    "        \"\"\"\n",
    "        Resample audio to 16Khz since that is being used by Whisper\n",
    "        \"\"\"\n",
    "        return lb.resample(x, orig_sr=sr, target_sr=16000)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        concat_audio = []\n",
    "        concat_label = []\n",
    "        \n",
    "        for i in range(idx*self.concat_n, (idx+1)*self.concat_n):\n",
    "            a = self.raw_ds[i][\"audio\"][\"array\"]\n",
    "            a = self._resample_audio(\n",
    "                a,\n",
    "                self.raw_ds[i][\"audio\"][\"sampling_rate\"]\n",
    "            )\n",
    "\n",
    "            concat_audio.append(a)\n",
    "            concat_label.append(self.raw_ds[i][\"sentence\"])\n",
    "\n",
    "        concat_audio = np.concatenate(concat_audio, axis=0)\n",
    "        if self.transform:\n",
    "            concat_audio = self.transform(concat_audio)\n",
    "            \n",
    "        return concat_audio, ' '.join(concat_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fa37e4-6c00-4c2a-8772-37f47a7f88d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = CustomAudioDataset(\n",
    "    raw_ds=common_voice[\"test\"],\n",
    "    concat_n=50,\n",
    "    transform=transform_cut_silences\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc73da6-3103-48d3-bb1a-b7f0f12e872d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    training_data, batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ef762a-64cf-42fe-baad-38006ad9e052",
   "metadata": {},
   "source": [
    "## Loading the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2339b17-cff4-4f3a-a293-ad396c6c6fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "transcriber = pipeline(\n",
    "    task=\"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    feature_extractor=feature_extractor,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb74eb3-1e88-421c-8904-ca0c725add24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wer_scores = []\n",
    "i = 0\n",
    "\n",
    "for audio, transc in tqdm(training_data, total=len(training_data)):\n",
    "    empty_cache()\n",
    "    generated_transc = transcriber(\n",
    "        audio,\n",
    "        chunk_length_s=config.get('chunk_length_s'),\n",
    "        return_timestamps=config.get('return_timestamps'),\n",
    "        generate_kwargs={\n",
    "            'forced_decoder_ids': config.get('forced_decoder_ids'),\n",
    "            'num_beams': config.get('num_beams')\n",
    "        }\n",
    "    )['text']\n",
    "\n",
    "    generated_transc_ids = transcriber.tokenizer.encode(generated_transc)\n",
    "    transc_ids = transcriber.tokenizer.encode(transc)\n",
    "    # Make sure both predictions and references are of the same length\n",
    "    if len(generated_transc_ids) > len(transc_ids):\n",
    "        pred_ids = generated_transc_ids[:len(transc_ids)]\n",
    "        ref_ids = transc_ids\n",
    "    else:\n",
    "        pred_ids = generated_transc_ids\n",
    "        ref_ids = transc_ids[:len(pred_ids)]\n",
    "\n",
    "    wer_score = wer_metric.compute(\n",
    "        predictions=transcriber.tokenizer.batch_decode(pred_ids, skip_special_tokens=True),\n",
    "        references=transcriber.tokenizer.batch_decode(ref_ids, skip_special_tokens=True)\n",
    "    )\n",
    "    wer_scores.append(wer_score)\n",
    "\n",
    "    i += 1\n",
    "\n",
    "    if i >= 10:\n",
    "        break\n",
    "\n",
    "wer_scores = np.array(wer_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb298a0-c734-4740-a7f2-069cb98929ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d362b99-9a2b-4972-a0b8-ccbe31ab9fc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
